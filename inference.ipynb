{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PIL import Image, ImageDraw\n",
    "from omegaconf import OmegaConf\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.VO import VOSampler\n",
    "import os \n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from copy import deepcopy\n",
    "import torch \n",
    "from ldm.util import instantiate_from_config\n",
    "from trainer import batch_to_device\n",
    "from inpaint_mask_func import draw_masks_from_boxes\n",
    "import numpy as np\n",
    "import clip \n",
    "from scipy.io import loadmat\n",
    "from functools import partial\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from box_utils import save_img, Pharse2idx_2, process_box_phrase, format_box, draw_box_2\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning import seed_everything\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from urllib.request import urlopen\n",
    "device = \"cuda\"\n",
    "\n",
    "def set_alpha_scale(model, alpha_scale):\n",
    "    from ldm.modules.attention import GatedCrossAttentionDense, GatedSelfAttentionDense\n",
    "    for module in model.modules():\n",
    "        if type(module) == GatedCrossAttentionDense or type(module) == GatedSelfAttentionDense:\n",
    "            module.scale = alpha_scale\n",
    "\n",
    "def alpha_generator(length, type=None):\n",
    "\n",
    "    if type == None:\n",
    "        type = [1,0,0]\n",
    "\n",
    "    assert len(type)==3 \n",
    "    assert type[0] + type[1] + type[2] == 1\n",
    "    \n",
    "    stage0_length = int(type[0]*length)\n",
    "    stage1_length = int(type[1]*length)\n",
    "    stage2_length = length - stage0_length - stage1_length\n",
    "    \n",
    "    if stage1_length != 0: \n",
    "        decay_alphas = np.arange(start=0, stop=1, step=1/stage1_length)[::-1]\n",
    "        decay_alphas = list(decay_alphas)\n",
    "    else:\n",
    "        decay_alphas = []\n",
    "        \n",
    "    \n",
    "    alphas = [1]*stage0_length + decay_alphas + [0]*stage2_length\n",
    "    \n",
    "    assert len(alphas) == length\n",
    "    \n",
    "    return alphas\n",
    "\n",
    "def load_ckpt(ckpt_path):\n",
    "    \n",
    "    saved_ckpt = torch.load(ckpt_path)\n",
    "    config = saved_ckpt[\"config_dict\"][\"_content\"]\n",
    "\n",
    "    model = instantiate_from_config(config['model']).to(device)\n",
    "    autoencoder = instantiate_from_config(config['autoencoder']).to(device).eval()\n",
    "    text_encoder = instantiate_from_config(config['text_encoder']).to(device).eval()\n",
    "    diffusion = instantiate_from_config(config['diffusion']).to(device)\n",
    "\n",
    "    # donot need to load official_ckpt for self.model here, since we will load from our ckpt\n",
    "    model.load_state_dict( saved_ckpt['model'] )\n",
    "    autoencoder.load_state_dict( saved_ckpt[\"autoencoder\"]  )\n",
    "    text_encoder.load_state_dict( saved_ckpt[\"text_encoder\"]  )\n",
    "    diffusion.load_state_dict( saved_ckpt[\"diffusion\"]  )\n",
    "\n",
    "    return model, autoencoder, text_encoder, diffusion, config\n",
    "\n",
    "def project(x, projection_matrix):\n",
    "\n",
    "    return x@torch.transpose(projection_matrix, 0, 1)\n",
    "\n",
    "def get_clip_feature(model, processor, input, is_image=False):\n",
    "    which_layer_text = 'before'\n",
    "    which_layer_image = 'after_reproject'\n",
    "\n",
    "    if is_image:\n",
    "        if input == None:\n",
    "            return None\n",
    "        image = Image.open(input).convert(\"RGB\")\n",
    "        inputs = processor(images=[image],  return_tensors=\"pt\", padding=True)\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].cuda() # we use our own preprocessing without center_crop \n",
    "        inputs['input_ids'] = torch.tensor([[0,1,2,3]]).cuda()  # placeholder\n",
    "        outputs = model(**inputs)\n",
    "        feature = outputs.image_embeds \n",
    "        if which_layer_image == 'after_reproject':\n",
    "            feature = project( feature, torch.load('projection_matrix').cuda().T ).squeeze(0)\n",
    "            feature = ( feature / feature.norm() )  * 28.7 \n",
    "            feature = feature.unsqueeze(0)\n",
    "    else:\n",
    "        if input == None:\n",
    "            return None\n",
    "        inputs = processor(text=input,  return_tensors=\"pt\", padding=True)\n",
    "        inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "        inputs['pixel_values'] = torch.ones(1,3,224,224).cuda() # placeholder \n",
    "        inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "        outputs = model(**inputs)\n",
    "        if which_layer_text == 'before':\n",
    "            feature = outputs.text_model_output.pooler_output\n",
    "    return feature\n",
    "\n",
    "def complete_mask(has_mask, max_objs):\n",
    "\n",
    "    mask = torch.ones(1,max_objs)\n",
    "    if has_mask == None:\n",
    "        return mask \n",
    "\n",
    "    if type(has_mask) == int or type(has_mask) == float:\n",
    "        return mask * has_mask\n",
    "    else:\n",
    "        for idx, value in enumerate(has_mask):\n",
    "            mask[0,idx] = value\n",
    "        return mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_batch(meta, batch=1, max_objs=30):\n",
    "\n",
    "    phrases, images = meta.get(\"phrases\"), meta.get(\"images\")\n",
    "    images = [None]*len(phrases) if images==None else images \n",
    "    phrases = [None]*len(images) if phrases==None else phrases \n",
    "\n",
    "    version = \"openai/clip-vit-large-patch14\"\n",
    "    model = CLIPModel.from_pretrained(version).cuda()\n",
    "    processor = CLIPProcessor.from_pretrained(version)\n",
    "\n",
    "    boxes = torch.zeros(max_objs, 4)\n",
    "    masks = torch.zeros(max_objs)\n",
    "    text_masks = torch.zeros(max_objs)\n",
    "    image_masks = torch.zeros(max_objs)\n",
    "    text_embeddings = torch.zeros(max_objs, 768)\n",
    "    image_embeddings = torch.zeros(max_objs, 768)\n",
    "    \n",
    "    text_features = []\n",
    "    image_features = []\n",
    "    for phrase, image in zip(phrases,images):\n",
    "        text_features.append(  get_clip_feature(model, processor, phrase, is_image=False) )\n",
    "        image_features.append( get_clip_feature(model, processor, image,  is_image=True) )\n",
    "\n",
    "    for idx, (box, text_feature, image_feature) in enumerate(zip( meta['locations'], text_features, image_features)):\n",
    "        boxes[idx] = torch.tensor(box)\n",
    "        masks[idx] = 1\n",
    "        if text_feature is not None:\n",
    "            text_embeddings[idx] = text_feature\n",
    "            text_masks[idx] = 1 \n",
    "        if image_feature is not None:\n",
    "            image_embeddings[idx] = image_feature\n",
    "            image_masks[idx] = 1 \n",
    "\n",
    "    out = {\n",
    "        \"boxes\" : boxes.unsqueeze(0).repeat(batch,1,1),\n",
    "        \"masks\" : masks.unsqueeze(0).repeat(batch,1),\n",
    "        \"text_masks\" : text_masks.unsqueeze(0).repeat(batch,1)*complete_mask( meta.get(\"text_mask\"), max_objs ),\n",
    "        \"image_masks\" : image_masks.unsqueeze(0).repeat(batch,1)*complete_mask( meta.get(\"image_mask\"), max_objs ),\n",
    "        \"text_embeddings\"  : text_embeddings.unsqueeze(0).repeat(batch,1,1),\n",
    "        \"image_embeddings\" : image_embeddings.unsqueeze(0).repeat(batch,1,1)\n",
    "    }\n",
    "\n",
    "    return batch_to_device(out, device) \n",
    "\n",
    "def crop_and_resize(image):\n",
    "    crop_size = min(image.size)\n",
    "    image = TF.center_crop(image, crop_size)\n",
    "    image = image.resize( (512, 512) )\n",
    "    return image\n",
    "\n",
    "\n",
    "def run(meta,models,info_files, p, starting_noise=None,iter_id=0, img_id=0, save=True,count=-1):\n",
    "    model, autoencoder, text_encoder, diffusion, config = models\n",
    "\n",
    "    grounding_tokenizer_input = instantiate_from_config(config['grounding_tokenizer_input'])\n",
    "    model.grounding_tokenizer_input = grounding_tokenizer_input\n",
    "    \n",
    "    grounding_downsampler_input = None\n",
    "    if \"grounding_downsampler_input\" in config:\n",
    "        grounding_downsampler_input = instantiate_from_config(config['grounding_downsampler_input'])\n",
    "\n",
    "    # - - - - - update config from args - - - - - # \n",
    "    config.update( vars(args) )\n",
    "    config = OmegaConf.create(config)\n",
    "\n",
    "    # - - - - - prepare batch - - - - - #\n",
    "\n",
    "    batch = prepare_batch(meta, config.batch_size)\n",
    "    context = text_encoder.encode(  [meta[\"prompt\"]]*config.batch_size  )\n",
    "    uc = text_encoder.encode( config.batch_size*[\"\"] )\n",
    "    with torch.no_grad():\n",
    "        if args.negative_prompt is not None:\n",
    "            uc = text_encoder.encode( config.batch_size*[args.negative_prompt] )\n",
    "\n",
    "    # - - - - - sampler - - - - - # \n",
    "    alpha_generator_func = partial(alpha_generator, type=meta.get(\"alpha_type\"))\n",
    "    sampler = VOSampler(diffusion, model, alpha_generator_func=alpha_generator_func, set_alpha_scale=set_alpha_scale)\n",
    "    steps = 50\n",
    "    inpainting_mask = z0 = None  \n",
    "    inpainting_extra_input = None \n",
    "\n",
    "    grounding_input = grounding_tokenizer_input.prepare(batch)\n",
    "    grounding_extra_input = None\n",
    "    if grounding_downsampler_input != None:\n",
    "        grounding_extra_input = grounding_downsampler_input.prepare(batch)\n",
    "\n",
    "    input = dict(\n",
    "\n",
    "                x = starting_noise, \n",
    "                timesteps = None, \n",
    "                context = context, \n",
    "                grounding_input = grounding_input,\n",
    "                inpainting_extra_input = None,\n",
    "                grounding_extra_input = grounding_extra_input,\n",
    "                boxes=meta['ll'],\n",
    "                object_position = meta['position'],\n",
    "            )\n",
    "\n",
    "    # - - - - - start sampling - - - - - #\n",
    "    shape = (config.batch_size, model.in_channels, model.image_size, model.image_size)\n",
    "    samples_fake,img_list,x0_list = sampler.sample(S=steps, shape=shape, input=input,  uc=uc, guidance_scale=config.guidance_scale, mask=inpainting_mask, x0=z0, loss_type=None)\n",
    "    with torch.no_grad():\n",
    "        samples_fake = autoencoder.decode(samples_fake)\n",
    "    for i in range(steps):\n",
    "        with torch.no_grad():\n",
    "            img_fake = autoencoder.decode(img_list[i])\n",
    "            x0_fake = autoencoder.decode(x0_list[i])\n",
    "        img_fake = torch.clamp(img_fake[0], min=-1, max=1) * 0.5 + 0.5\n",
    "        img_fake = img_fake.cpu().numpy().transpose(1, 2, 0) * 255\n",
    "        img_fake = Image.fromarray(img_fake.astype(np.uint8))\n",
    "\n",
    "        x0_fake = torch.clamp(x0_fake[0], min=-1, max=1) * 0.5 + 0.5\n",
    "        x0_fake = x0_fake.cpu().numpy().transpose(1, 2, 0) * 255\n",
    "        x0_fake = Image.fromarray(x0_fake.astype(np.uint8))\n",
    "\n",
    "    # save images\n",
    "    if save :\n",
    "        #path = meta[\"save_folder_name\"]\n",
    "        output_folder1 = os.path.join( args.folder,  meta[\"save_folder_name\"]+'_img')\n",
    "        os.makedirs( output_folder1, exist_ok=True)\n",
    "        output_folder2 = os.path.join( args.folder,  meta[\"save_folder_name\"] + '_box')\n",
    "        os.makedirs( output_folder2, exist_ok=True)\n",
    "        start = len( os.listdir(output_folder2) )\n",
    "        image_ids = list(range(start,start+config.batch_size))\n",
    "        print(image_ids)\n",
    "        font = ImageFont.truetype(\"Roboto-LightItalic.ttf\", size=20)\n",
    "        for image_id, sample in zip(image_ids, samples_fake):\n",
    "            img_name = meta['prompt'].replace(' ', '_') + str(int(image_id))+'.png'\n",
    "            sample = torch.clamp(sample, min=-1, max=1) * 0.5 + 0.5\n",
    "            sample = sample.cpu().numpy().transpose(1,2,0) * 255 \n",
    "            sample = Image.fromarray(sample.astype(np.uint8))\n",
    "            img2 = sample.copy()\n",
    "            draw = ImageDraw.Draw(sample)\n",
    "            boxes = meta['location_draw']\n",
    "            text = meta[\"phrases\"]\n",
    "            \n",
    "            info_files.update({img_name: (text, boxes)})\n",
    "            for i, box in enumerate(boxes):\n",
    "                t = text[i]\n",
    "\n",
    "                draw.rectangle([(box[0], box[1]),(box[2], box[3])], outline=128, width=2)\n",
    "                draw.text((box[0]+5, box[1]+5), t, fill=200,font=font )\n",
    "            save_img(output_folder2, sample,meta['prompt'],iter_id,img_id,count)\n",
    "            save_img(output_folder1,img2,meta['prompt'],iter_id ,img_id,count)\n",
    "            \n",
    "    return samples_fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--folder\", type=str,  default=\"visual\")\n",
    "parser.add_argument('--ckpt', type=str, default='gligen_checkpoints/diffusion_pytorch_model.bin')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument(\"--guidance_scale\", type=float,  default=7.5)\n",
    "parser.add_argument(\"--negative_prompt\", type=str,  default='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
    "parser.add_argument(\"--file_save\", type=str, default='result')\n",
    "args = parser.parse_args('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckpt = args.ckpt = 'gligen_checkpoints/diffusion_pytorch_model.bin'\n",
    "file_save = args.file_save = 'result'\n",
    "batch_size = args.batchsize =  1\n",
    "\n",
    "meta_list = [ \n",
    "\n",
    "    # - - - - - - - - GLIGEN on text grounding for generation - - - - - - - - # \n",
    "    dict(\n",
    "        ckpt = ckpt,\n",
    "        prompt =None,\n",
    "        phrases = None,\n",
    "        locations = None,\n",
    "        alpha_type = [0.3, 0.0, 0.7],\n",
    "        save_folder_name=file_save,\n",
    "        ll = None\n",
    "    )\n",
    "]\n",
    "\n",
    "models = load_ckpt(meta_list[0][\"ckpt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position [[10], [2], [5]] A car and a bike in front of a house.\n",
      "step  0\n",
      "optimize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/ldm_layout/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  1\n",
      "optimize 1\n",
      "step  2\n",
      "optimize 2\n",
      "step  3\n",
      "optimize 3\n",
      "step  4\n",
      "optimize 4\n",
      "step  5\n",
      "optimize 5\n",
      "step  6\n",
      "optimize 6\n",
      "step  7\n",
      "optimize 7\n",
      "step  8\n",
      "optimize 8\n",
      "step  9\n",
      "optimize 9\n",
      "step  10\n",
      "optimize 10\n",
      "step  11\n",
      "optimize 11\n",
      "step  12\n",
      "optimize 12\n",
      "step  13\n",
      "optimize 13\n",
      "step  14\n",
      "optimize 14\n",
      "step  15\n",
      "optimize 15\n",
      "step  16\n",
      "optimize 16\n",
      "step  17\n",
      "optimize 17\n",
      "step  18\n",
      "optimize 18\n",
      "step  19\n",
      "optimize 19\n",
      "step  20\n",
      "optimize 20\n",
      "step  21\n",
      "optimize 21\n",
      "step  22\n",
      "optimize 22\n",
      "step  23\n",
      "optimize 23\n",
      "step  24\n",
      "optimize 24\n",
      "step  25\n",
      "optimize 25\n",
      "step  26\n",
      "optimize 26\n",
      "step  27\n",
      "optimize 27\n",
      "step  28\n",
      "optimize 28\n",
      "step  29\n",
      "optimize 29\n",
      "step  30\n",
      "optimize 30\n",
      "step  31\n",
      "optimize 31\n",
      "step  32\n",
      "optimize 32\n",
      "step  33\n",
      "optimize 33\n",
      "step  34\n",
      "optimize 34\n",
      "step  35\n",
      "optimize 35\n",
      "step  36\n",
      "optimize 36\n",
      "step  37\n",
      "optimize 37\n",
      "step  38\n",
      "optimize 38\n",
      "step  39\n",
      "optimize 39\n",
      "step  40\n",
      "optimize 40\n",
      "step  41\n",
      "optimize 41\n",
      "step  42\n",
      "optimize 42\n",
      "step  43\n",
      "optimize 43\n",
      "step  44\n",
      "optimize 44\n",
      "step  45\n",
      "optimize 45\n",
      "step  46\n",
      "optimize 46\n",
      "step  47\n",
      "optimize 47\n",
      "step  48\n",
      "optimize 48\n",
      "step  49\n",
      "optimize 49\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "caption = 'A car and a bike in front of a house.'\n",
    "names_list = ['house','car','bike']\n",
    "layout = [(66, 197, 452, 390), (326, 358, 402, 432), (111, 347, 216, 431)] \n",
    "\n",
    "info_files = {}\n",
    "\n",
    "for meta in meta_list:\n",
    "\n",
    "    pp = caption\n",
    "    o_names = names_list\n",
    "    o_boxes = layout\n",
    "    meta[\"prompt\"] = pp \n",
    "    text = pp \n",
    "    \n",
    "    for k in range(1):\n",
    "\n",
    "        starting_noise = torch.randn(batch_size, 4, 64, 64).to(device)\n",
    "        starting_noise = starting_noise.to(device)\n",
    "\n",
    "        p, ll  = format_box(o_names, o_boxes)\n",
    "        l = np.array(o_boxes)\n",
    "        name_box = process_box_phrase(o_names, o_boxes)\n",
    "        position, box_att = Pharse2idx_2(pp, name_box)\n",
    "        print('position', position, pp )\n",
    "        meta[\"phrases\"] = p\n",
    "        meta['location_draw'] = l\n",
    "        meta[\"locations\"] = l/512\n",
    "        meta['ll'] = box_att\n",
    "        meta['position'] = position\n",
    "        run(meta, models, info_files, args, starting_noise, k,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm_layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
